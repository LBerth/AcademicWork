{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Stochastic Approximation</div>\n",
    "\n",
    "L'objectif de ce notebook est de vous donner l'occasion de comprendre plus en profondeur le principe de la Descente de Gradient Stochastique qui permet la mise à jour des poids d'un réseau de neurones. \n",
    "\n",
    "1. [Rappel : Descente de Gradient](#sec1)\n",
    "2. [Exemple : réseau de neurones simplifié](#sec2)\n",
    "3. [Approximation Stochastique](#sec3)\n",
    "5. [Pour aller plus loin...](#sec4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec1\"></a>Rappels : Descente de Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par quelques rappels sur le fonctionnement de la descente de gradient classique.\n",
    "\n",
    "Tout d'abord, nous voulons trouver le minimum de la fonction suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def function(a):\n",
    "    X = [-1, 0, 1]\n",
    "    return sum([(2*np.sin(x)+1 - a[0] - a[1]*x)**2 for x in X])\n",
    "\n",
    "def gradient(a):\n",
    "    X = [-1, 0, 1]\n",
    "    grad = np.zeros_like(a)\n",
    "    grad[0] = sum([-2*(2*np.sin(x)+1-a[0]-a[1]*x) for x in X])\n",
    "    grad[1] = sum([-2*(2*np.sin(x)+1-a[0]-a[1]*x)*x for x in X])\n",
    "    return grad\n",
    "\n",
    "def display_func(R, func):\n",
    "    fig = plt.figure(figsize = (20,7))\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    x, y = np.linspace(-3, 3, 100), np.linspace(-3, 3, 100)\n",
    "    z = np.array([[func([a, b]) for b in y] for a in x])\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    ax1.plot_wireframe(x, y, z)\n",
    "    ax1.set_xlabel(\"a[0]\")\n",
    "    ax1.set_ylabel(\"a[1]\")\n",
    "    ax1.set_zlabel(\"f(a)\")\n",
    "\n",
    "    if R != []:\n",
    "        ax1.plot([r[0] for r in R], [r[1] for r in R], [func(r) for r in R], 'r-')\n",
    "\n",
    "        ax2 = fig.add_subplot(1, 2, 2)\n",
    "        ax2.plot([func(r) for r in R], 'g-')\n",
    "        ax2.set_xlabel(\"Number of steps\")\n",
    "        ax2.set_ylabel(\"f(x*)\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_func([], function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre cas, une première possibilité serait de calculer le gradient de la fonction et de trouver analytiquement en quel point il s'annule afin d'obtenir une solution exacte du problème.\n",
    "\n",
    "Malheureusement, même si nous savons calculer le gradient de la fonction en tout point, dans certains cas il est difficile voire impossible d'évaluer en quel point il s'annule. Nous allons donc procéder différement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Méthode de la descente de gradient à pas fixe :**\n",
    "\n",
    "Soient $\\mathbb{E} (\\langle\\cdot,\\cdot\\rangle)$ un espace hilbertien et $x\\in\\mathbb{E}\\mapsto f(x)\\in \\mathbb{R}$ une fonction différentiable. On note  $\\nabla f(x)$ le gradient de $f$ en $x$.\n",
    "\n",
    "**Théorème | Algorithme du gradient** On se donne un point initial $x_0\\in\\mathbb{E}$ et un seuil de tolérance $\\varepsilon > 0$. L'algorithme du gradient définit une suite $x_1, x_2, \\ldots \\in\\mathbb{E}$, jusqu'à ce qu'un test d'arrêt soit satisfait. Il passe de $x_k$ à $x_{k+1}$ par les étapes suivantes :\n",
    "- *Simulation* : calcul de $\\nabla f(x_k)$.\n",
    "- *Calcul du pas* $\\alpha_k>0$ par une règle de recherche linéaire sur $f$ en $x_k$ le long de la direction $-\\nabla f(x_k)$.\n",
    "- *Nouvel itéré* : $x_{k+1} = x_k - \\alpha_k \\nabla f(x_k).$\n",
    "- *Test d'arrêt*\n",
    "\n",
    "Pour une fonction strictement convexe, il est possible de montrer la convergence de cet algorithme vers l'unique minimum $x^*$ de $f$ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice : </b><br>\n",
    "Calculer une approximation du minimum de la fonction précédente grâce à une decente de gradient à pas fixe. Quelles sont les différentes conditions d'arrêt possibles ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code1.py\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "def fixed_step_descent(function, gradient, eps, step_size):\n",
    "    x = [...]\n",
    "    result = [x.copy()]  # At each step of the algorithm, add the value of x in result to get the display\n",
    "    \n",
    "    ## WRITE YOUR CODE HERE\n",
    "    \n",
    "    return result\n",
    "\n",
    "result = fixed_step_descent(function, gradient, ... , ... )\n",
    "print(\"Le minimum obtenu est : \", round(function(result[-1]),5), \" pour a = \", result[-1])\n",
    "\n",
    "display_func(result, function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malheureusement, cette méthode peut rencontrer de nombreux problèmes, par exemple lorsque le minimum se trouve au fond d'une vallée étroite, la convergence est parfois impossible. De plus, la recherche du pas optimal peut être longue. \n",
    "\n",
    "Il existe d'autres algorithmes qui offrent souvent de meilleurs résultats, comme par exemple :\n",
    "- la **méthode de Newton** qui définit la suite des $x_k$ de la façon suivante : $x_{k+1} = x_k - \\frac{f(x_k)}{\\nabla f(x_k)}$\n",
    "- la **méthode BFGS** qui consiste à calculer en chaque étape une matrice, qui multipliée au gradient permet d'obtenir une meilleure direction. De plus, cette méthode peut être combinée avec une méthode plus efficace de recherche linéaire afin d'obtenir la meilleure valeur de α. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappelons toutefois que ces différentes méthodes fonctionnent bien pour des fonctions convexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boulders = 150\n",
    "boulders = np.random.uniform(0., 1., (n_boulders, 3))\n",
    "boulders[:, 2] = 1/(3 ** np.random.choice(range(2, 4), (n_boulders)))\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n",
    "Z = np.zeros(X.shape)\n",
    "\n",
    "for i in range(n_boulders):\n",
    "    eval_ = (X - boulders[i, 0]) ** 2 + (Y - boulders[i, 1]) ** 2 - boulders[i, 2] ** 2\n",
    "    Z[eval_ < 0] += eval_[eval_<0]\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.contour(X, Y, Z, alpha=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un cas non convexe comme le précédent, on pourrait appliquer plusieurs fois la méthode en partant de différents points de départ afin d'explorer tous les puits de potentiel. Il serait alors possible de trouver les différents points de départ par une méthode métaheuristique, comme un algorithme génétique ou un recuit simulé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec2\"></a>Exemple : réseau de neurones simplifié"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changeons un peu de contexte : nous nous plaçons maintenant dans le cas où nous voulons approximer une fonction inconnue par une fonction linéaire de type $y = a_0 + a_1*x$. Cependant, nous connaissons seulement la valeur de la fonction en quelques points $(x_i, y_i)$. Pire, ces échantillons sont bruités ! \n",
    "\n",
    "On pourra rapprocher cet exemple du cas de la **mise à jour des poids d'un réseau de neurones**. Ici on a un réseau hyper-simplifié, avec seulement un poids, $a_1$, et un biais, $a_0$, et à terme on veut qu'il puisse nous prédire la valeur de $y$ pour un échantillon $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(n):\n",
    "    samples = []\n",
    "    for i in range(n):\n",
    "        x = random.uniform(-1, 1)\n",
    "        samples.append([x, 2*np.sin(x)+1 + 0.1*np.random.randn()])\n",
    "    return np.array(samples)\n",
    "\n",
    "n = 100\n",
    "samples = generate_samples(n)\n",
    "X = samples[:,:-1]\n",
    "X = np.c_[np.ones(n), X]    # add a column of ones at the beginning\n",
    "Y = samples[:,-1]\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "plt.plot(samples[:,0], samples[:,1], 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïvement, notre première idée est de trouver $a_0$ et $a_1$ qui minimisent la somme des carrés des erreurs $y_i - (a_0 + a_1 * x_i)$ :\n",
    "\n",
    "$$ \\min_{(a_0, a_1) \\in \\mathbb{R}^2} \\sum_{i=1}^{N} (y_i - a_0 - a_1 * x_i)^2 $$\n",
    "\n",
    "La première solution possible consiste à calculer sous forme matricielle la solution exacte de ce problème grâce à la formule suivante : \n",
    "\n",
    "$$ \\hat{a} = (X^{T}X)^{-1}X^{T}Y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice : </b> Appliquez cette formule pour trouver les $a_0$ et $a_1$ optimaux.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code2.py\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "a = ... ## WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que cette méthode nous fournisse la meilleure solution possible, elle nécessite d'inverser une matrice de taille N. Dans notre cas, nous avons peu d'échantillons mais avec un *dataset* de plusieurs millions d'entrées, une telle inversion nous coûterait cher en temps de calcul et en mémoire.\n",
    "\n",
    "Une autre possibilité est donc de reprendre la méthode précédente de **descente de gradient**.\n",
    "\n",
    "La fonction à minimiser est dorénavent la fonction de coût, ou *loss function* :\n",
    "$$  L(a) = \\sum_{i=1}^{N} (y_i - a_0 - a_1 * x_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice : </b> Codez la <i>loss function</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load solutions/code3.py\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "def loss(a):\n",
    "    return ## WRITE YOUR CODE HERE\n",
    "\n",
    "display_func([], loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons notre fonction à minimiser, nous pouvons à nouveau appliquer la méthode du gradient à pas fixe grâce à la même formule que précédemment : \n",
    "$$ a_{k+1} = a_k - \\beta_k \\nabla f(a_k) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice : </b> Calculez $\\nabla L$ puis implémentez la descente de gradient à pas fixe.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code4.py\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "def loss_gradient(a,X,Y):\n",
    "    grad = [0]*X.shape[1]\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return grad\n",
    "\n",
    "def gradient_descent(X,Y, eps, step_size):\n",
    "    a = [0]*X.shape[1]\n",
    "    result = [a.copy()]\n",
    "    \n",
    "    # WRITE YOUR CODE HERE\n",
    "        \n",
    "    return result\n",
    "\n",
    "result = gradient_descent(X,Y, ... , ...)\n",
    "\n",
    "print(\"Le minimum obtenu est : \", round(loss(result[-1]),5), \" pour a = \", result[-1])\n",
    "\n",
    "display_func(result, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Très bien ! Cette méthode nous a donc permis d'obtenir une bonne approximation du minimum de notre *loss function*. Mais est-ce que vous ne flairez pas une arnaque ? C'était un peu trop simple, non ?\n",
    "\n",
    "Le problème ici, c'est que l'objectif d'un réseau de neurones, ce n'est pas simplement de faire une régression linéaire entre des points, mais de pouvoir reproduire la fonction qui est à l'origine de ces points, sinon on dit qu'on *overfit* le dataset. Le *dataset* que nous utilisons étant un nombre fini de points bruités, il nous permet seulement d'obtenir une **approximation** de la *loss function*, et donc en minimisant celle-ci avec la descente de gradient classique, nous ne sommes pas sûrs de converger vers la vraie fonction que nous recherchons.\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "Réécrivons tout cela de façon un peu plus mathématique :\n",
    "\n",
    "Nous avons un dataset de points $(x, y)$ tirés d'une distribution de probabilité $p(x, y)$. Apprendre un réseau de neurones qui prédit correctement $y$ en fonction de $x$ correspond à trouver les paramètres $\\theta$ qui minimisent la fonction suivante :\n",
    "\n",
    "$$L(\\theta) = \\displaystyle \\mathbb{E}_{(x,y)\\sim p(x,y)} \\left[ \\left(f_\\theta(x) - y\\right)^2 \\right] = \\int_{x,y} \\left[ \\left(f_\\theta(x) - y\\right)^2 \\right] \\mathrm{d}p(x,y) $$\n",
    "\n",
    "Ceci est la vraie loss function que nous voulons minimiser. La descente de gradient classique nous dit qu'il faut se déplacer dans la direction opposée au gradient de $L(\\theta)$ pour trouver $\\theta$ optimal. Ecrivons ce gradient :\n",
    "\n",
    "\\begin{align*}\n",
    "\\displaystyle \\nabla_\\theta L(\\theta) &= \\nabla_\\theta \\left[ \\mathbb{E}_{(x,y)\\sim p(x,y)} \\left[ \\left(f_\\theta(x) - y\\right)^2 \\right] \\right]\\\\\n",
    "&= \\mathbb{E}_{(x,y)\\sim p(x,y)} \\left[ \\nabla_\\theta \\left[ \\left(f_\\theta(x) - y\\right)^2 \\right] \\right]\\\\\n",
    "&= \\mathbb{E}_{(x,y)\\sim p(x,y)} \\left[ 2 \\left(f_\\theta(x) - y\\right) \\nabla_\\theta f_\\theta(x) \\right]\n",
    "\\end{align*}\n",
    "\n",
    "Donc, le gradient de $L(\\theta)$ est l'espérance de $2 \\left(f_\\theta(x) - y\\right) \\nabla_\\theta f_\\theta(x)$. En d'autres mots :\n",
    "\n",
    "$$\\nabla_\\theta L(\\theta) = \\int_{x,y} 2 \\left(f_\\theta(x) - y\\right) \\nabla_\\theta f_\\theta(x) \\mathrm{d}p(x,y)$$\n",
    "\n",
    "Le problème avec cette expression est que nous avons besoin de connaître $p(x,y)$ pour toutes les paires $(x,y)$ possibles. Pour cela il nous faudrait une quantité infinie de donnée. On peut cenpendant approximer ce gradient avec un dataset fini $\\left\\{\\left(x_i,y_i\\right)\\right\\}_{i\\in [1,N]}$:\n",
    "$$\\nabla_\\theta L(\\theta) \\approx \\sum_{i=1}^N 2 \\left(f_\\theta(x_i) - y_i\\right) \\nabla_\\theta f_\\theta(x_i)$$\n",
    "\n",
    "Ici nous avons une *estimation bruitée du gradient* , qui converge vers le vrai gradient lorsque le nombre d'échantillons tend vers l'infini. C'est cette approximation que nous avons utilisé plus tôt avec la descente de gradient classique. Toutefois, même si cette approximation du gradient converge vers le vrai gradient pour un nombre d'échantillons infini, nous n'avons aucune preuve qu'avec cette approximation la descente de gradient permettra à notre réseau de converger vers la vraie fonction.\n",
    "\n",
    "Pour résoudre ce problème, nous avons besoin d'un outil supplémentaire : l'**approximation stochastique** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec3\"></a>Approximation Stochastique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'algorithme de Robbins-Monro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode de l'approximation stochastique fut inventée par Herbert Robbins et Sutton Monro et publiée en 1951 dans l'article fondateur [*A Stochastic Approximation Method*](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586)\n",
    "\n",
    "**Descriptions de l'algorithme :**\n",
    "\n",
    "Soit $M$ une fonction et $\\alpha$ une constante telle que l'équation $M(\\theta) = \\alpha$ possède une unique racine en $\\theta^*$.\n",
    "On suppose que l'on ne peut pas observer directement les valeurs de la fonction $M$, mais que nous pouvons cependant obtenir des mesures de la variable aléatoire $N(\\theta)$ telle que $E[N(\\theta)] = M(\\theta)$. On veut estimer $\\theta^*$ en faisant des observation successives de $N$ en $\\theta_1$, $\\theta_2$ , $\\theta_3$, telles que la suite des $\\theta_n$ soit de la forme :\n",
    "$$ \\theta_{n+1} = \\theta_{n} - a_{n} (N(\\theta_{n}) - \\alpha) $$\n",
    "\n",
    "où $(a_{n})$ est une suite de pas positifs. \n",
    "\n",
    "\n",
    "**Hypothèses de convergence :**\n",
    "\n",
    "Robbins et Monro ont montré que la suite des $\\theta_n$ converge en $L^2$ vers $\\theta^*$ si les conditions suivantes sont remplies :\n",
    "* $N(\\theta)$ est bornée\n",
    "* $M$ est croissante\n",
    "* $M'(\\theta^*)$ existe et est positive\n",
    "* la suite des $a_n$ vérifie $$\\qquad \\sum^{\\infty}_{n=0}a_n = \\infty \\quad \\mbox{ and } \\quad \\sum^{\\infty}_{n=0}a^2_n < \\infty \\quad $$\n",
    "\n",
    "Une suite particulière a été proposée par Robbins et Monro : $a_n = \\frac{a}{n}$ avec $a > 0$.\n",
    "\n",
    "**Explications :**\n",
    "\n",
    "La preuve formelle de la convergence se trouve dans l'article [*A Stochastic Approximation Method*](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586), mais voici quelques explications intuitives sur les conditions imposées par l'algorithme :\n",
    "* Le fait que $N(\\theta)$ soit bornée impose que le bruit sur les échantillons soit borné, donc non biaisé et identique sur l'ensemble des $x_i$.\n",
    "* Les conditions 2 et 3 imposent que la fonction $M$ ait un comportement \"raisonnable\" près de sa racine et à l'infini.\n",
    "* $\\sum^{\\infty}_{n=0}a_n = \\infty$ nous dit que quelque soit le point de départ $\\theta_0$, même s'il est loin du minimum, l'algorithme pourra atteindre $\\theta^*$.\n",
    "* $\\sum^{\\infty}_{n=0}a^2_n < \\infty$ oblige le pas de l'algorithme à décroître et évite les oscillations autour du minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application à l'optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La théorie de la *descente de gradient stochastique* nous dit donc que si nous avons $g(\\theta)$, un estimateur bruité de $\\nabla_\\theta L(\\theta)$, alors la suite $\\theta_k$ converge vers un minimum local de $L(\\theta)$, pour\n",
    "$$\\theta_{k+1} = \\theta_k - \\alpha_k g(\\theta_k)$$\n",
    "avec les conditions  $\\sum \\alpha_k = \\infty$ et $\\sum \\alpha_k^2 < \\infty$ (appelées conditions de Robbins-Monro).\n",
    "\n",
    "---------------------\n",
    "\n",
    "Pour revenir à la mise à jour des poids d'un réseau de neurones, nous avons donc $g(\\theta)$, une fonction localement convexe qui approxime le gradient $\\nabla_\\theta L(\\theta)$ :\n",
    "$$g(\\theta) = \\sum_{i=1}^N 2 \\left(f_\\theta(x_i) - y_i\\right) \\nabla_\\theta f_\\theta(x_i)$$\n",
    "Pour appliquer l'algorithme nous allons utiliser la suite conseillée par Robbins et Monro : $\\alpha_n = \\frac{\\alpha}{n}$ avec $\\alpha > 0$.\n",
    "\n",
    "$\\alpha$ est aussi appelé le *learning rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice : </b> Reprenez les fonctions précédentes pour $L(\\theta)$ et son gradient et appliquez la méthode de l'approximation stochastique. Attention, l'algorithme est très sensible au choix de $\\alpha$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code5.py\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "def stochastic_gradient_descent(X,Y,lr,eps):\n",
    "    a = [0]*X.shape[1]\n",
    "    result = [a.copy()]\n",
    "    \n",
    "    # WRITE YOUR CODE HERE\n",
    "        \n",
    "    return result\n",
    "\n",
    "result = stochastic_gradient_descent(X,Y, ... , ...)\n",
    "\n",
    "print(\"Le minimum obtenu est : \", round(loss(result[-1]),5), \" pour a = \", result[-1])\n",
    "\n",
    "display_func(result, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà ! Nous avons donc une belle descente de gradient stochastique fonctionnelle !\n",
    "Il ne nous manque plus qu'une dernière astuce pour améliorer les performances de l'algorithme et nous aurons retrouvé l'algorithme qui est aujourd'hui couramment utilisé pour la mise à jour des poids d'un réseau de neurones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de Robbins-Monro nous impose peu de conditions sur l'estimateur de $L(\\theta)$. Jusqu'à présent nous l'avons calculé de la façon suivante : \n",
    "$$g(\\theta) = \\sum_{i=1}^N 2 \\left(f_\\theta(x_i) - y_i\\right) \\nabla_\\theta f_\\theta(x_i)$$\n",
    "\n",
    "Mais rien ne nous oblige à sommer sur l'ensemble du dataset ! Si on ne calcule la *loss* que sur un sous-ensemble aléatoire de taille $n$ du dataset nous aurons toujours un estimateur convenable de la *loss*, même s'il sera alors plus bruité. Cette astuce nous permettra surtout d'économiser beaucoup de temps de calcul lors de l'évaluation de $g(\\theta)$. \n",
    "\n",
    "Lorsque $n=1$, l'estimation du gradient est basée sur un seul échantillon, elle est donc très bruitée, et la convergence peut être lente et instable. Lorsque $n \\rightarrow N$, le niveau de bruit décroît mais le temps de calcul est plus long. En pratique, une valeur de $n$ entre 32 et 1024 fonctionne assez bien pour de grands *datasets*. (Généralement, on aime bien utiliser des puissances de 2.) \n",
    "\n",
    "On appelle un tel sous-ensemble du *dataset* un *minibatch*. Pendant l'execution de l'algorithme, nous allons donc découper le *dataset* en *minibatch* et le parcourir plusieurs fois. Un parcours entier du *dataset* est nommé une *epoch*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice : </b> Implémentez la <i>minibatch gradient descent</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/code6.py\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "def mini_batch_SGD(X,Y,lr, nb_epoch, batch_size):\n",
    "    a = [0]*X.shape[1]\n",
    "    result = [a.copy()]\n",
    "    \n",
    "    # WRITE YOUR CODE HERE\n",
    "    \n",
    "    return result\n",
    "\n",
    "result = mini_batch_SGD(X,Y, ... , ... , ...)\n",
    "\n",
    "print(\"Le minimum obtenu est : \", round(loss(result[-1]),5), \" pour a = \", result[-1])\n",
    "\n",
    "display_func(result, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour aller plus loin..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nombreuses améliorations sur l'algorithme SGD initial ont été proposées et utilisées.\n",
    "En particulier, la nécessité de fixer un taux d'apprentissage a souvent été décrite comme problématique. Un choix de paramètre trop grand peut rendre l'algorithme divergent tandis qu'un choix trop petit rend la convergence trop lente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moyennage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pratique, l'algorithme de Robbins-Monro est très sensible aux paramètres comme le point de départ $\\theta_0$ ou la calibration de la suite de pas $(\\alpha_n)$. La vitesse optimale de l'algorithme est atteinte pour un pas de la forme $\\alpha_n=\\alpha/n$ mais le choix de la constante $\\alpha$ est extrêmement important et la déterminer en pratique s'avère délicat. \n",
    "\n",
    "Pour contourner ce choix, Polyak et Juditsky proposèrent en 1992 dans [**Acceleration of Stochastic Approximation**](http://www.meyn.ece.ufl.edu/archive/spm_files/Courses/ECE555-2011/555media/poljud92.pdf) une version «moyennisée» de cet algorithme en prenant la suite de pas $(\\alpha_n)$ à décroissance plus lente que la vitesse «optimale», par exemple $\\alpha_n=n^{-3/4}$. Ils lissent ensuite la suite d'estimateurs $\\theta_{n+1} - \\theta_n = a_n(\\alpha - N(\\theta_n))$ en considérant le «moyennisé» : $$\\bar{\\theta}_n = \\frac{1}{n} \\sum^{n-1}_{i=0} \\theta_i$$ \n",
    "On peut alors montrer que $\\bar{\\theta}_n$ converge vers l'unique racine $\\theta^*$. En pratique, cet algorithme est beaucoup moins sensible au choix des paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La *méthode du moment* apparaît dans l'article [**Learning representations by back-propagating errors**](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf) de Rumelhart, Hinton et Williams en 1986 sur l'apprentissage par rétro-propagation.\n",
    "La méthode SGD avec moment conserve en mémoire la mise-à-jour à chaque étape ($\\Delta \\theta$), et calcule la suivante comme une combinaison convexe du gradient actuel et de la modification précédente :\n",
    "\n",
    "$$\\Delta \\theta = \\eta \\nabla g(\\theta) + \\alpha \\Delta \\theta$$\n",
    "$$\\theta = \\theta - \\eta \\Delta \\theta $$\n",
    "\n",
    "Le nom *moment* vient d'une analogie avec le moment en physique : le vecteur de paramètres $\\theta$, considéré comme une particule qui voyage au travers de l'espace des paramètres (souvent en grande dimension), subit une accélération via le gradient (qui agit comme une force physique). Contrairement à la méthode SGD classique, cette variante a tendance à continuer de *voyager* dans la même direction, en empêchant les oscillations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*AdaGrad* (ou *Adaptive Gradient Algorithm*) est une amélioration de la méthode SGD qui va déterminer automatiquement un taux d'apprentissage pour chaque paramètre. Cette méthode a été présentée en 2011 dans [**Adaptive Subgradient Methods for Online Learning and Stochastic Optimization**](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf). On doit toujours choisir un taux d'apprentissage commun ($\\eta$), mais celui-ci est multiplié par les éléments du vecteur $G_{j,j}$, qui est obtenu comme diagonale de la matrice G suivante :\n",
    "\n",
    "$$G = \\sum_{\\tau=1}^t g_\\tau g_\\tau^\\mathsf{T}$$\n",
    "\n",
    "où $g_\\tau = \\nabla Q_i(\\theta)$ est le gradient à l'étape $\\tau$.\n",
    "La diagonale est donnée par : $G_{j,j} = \\sum_{\\tau=1}^t g_{\\tau,j}^2$.\n",
    "\n",
    "Ce vecteur est mis-à-jour à chaque itération.\n",
    "Ainsi, la formule pour chaque mise-à-jour pour chaque paramètre est désormais :\n",
    "\n",
    "$$ \\theta_j = \\theta_j - \\frac{\\eta}{\\sqrt{G_{j,j}}} g_j$$\n",
    "\n",
    "Chaque $G_{j,j}$ donne un facteur multiplicatif appliqué au taux d'apprentissage correspondant à l'unique paramètre $\\theta_i$.\n",
    "Et comme le dénominateur de ce facteur ($\\sqrt{G_i} = \\sqrt{\\sum_{\\tau=1}^t g_\\tau^2}$) est la norme euclidienne usuelle des dérivées précédentes, les mises-à-jour trop importantes des paramètres sont atténuées tandis que les petites modifications sont faites avec un taux d'apprentissage plus grand (et donc sont amplifiées)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de Kiefer-Wolfowitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de Kiefer-Wolfowitz a été introduit en 1952 dans [**Stochastic estimation of the maximum of a regression function**](https://projecteuclid.org/euclid.aoms/1177729392) pour estimer le maximum d'une fonction $f:x↦E[F(x,ξ)]$\n",
    ", connue à une perturbation près. Lorsque le gradient de $f$ n'est pas observable, cet algorithme permet d'approcher le maximum en utilisant une approximation du gradient de $f$ par des différences finies. Il est défini pour $n≥1$, par l'itération :\n",
    "\n",
    "$$X_{n+1}=X_n−γ_{n+1} \\frac{F(X_n+c_n,ξ^1_n)−F(x_n−c_n,ξ^2_n)}{2c_n}$$\n",
    "\n",
    "où $(ξ^1_n)_n$ et $(ξ^2_n)_n$ sont deux suites de variables aléatoires indépendantes et identiquement distribuées, $(c_n)$ et $(γ_n)$ sont deux suites déterministes positives et décroissantes vers 0 vérifiant:\n",
    "\n",
    "$$∑nγ_n=∞, ∑nc_nγ_n<∞, ∑n(γ_n/c_n)^2<∞$$\n",
    "\n",
    "La suite $(c_n)$ désigne les largeurs des différences finies utilisées pour l'approximation du gradient, tandis que la suite $(γ_n)$ représente les pas de la descente du gradient. Sous de bonnes hypothèses de régularité et convexité/concavité de la fonction $f$, cet algorithme converge vers le maximum/minimum visé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liens et Références"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Wikipédia - Stochastic Approximation](https://en.wikipedia.org/wiki/Stochastic_approximation)\n",
    "* [Optimization Methods for Large-Scale Machine Learning](https://arxiv.org/pdf/1606.04838.pdf)\n",
    "* [Introduction to Stochastic Approximation Algorithms](http://www.professeurs.polymtl.ca/jerome.le-ny/teaching/DP_fall09/notes/lec11_SA.pdf)\n",
    "* [Wikipédia - Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "* [Algorithmes Stochastiques](http://cenac.perso.math.cnrs.fr/hdr/algo-stochastiques.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "236.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
